{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have already defined the label_dict and other parameters for the dataset\n",
    "DATA_PATH = \"/data/home/vvaibhav/AI/VAE/afhq\"\n",
    "width = 128\n",
    "batch_size = 1024\n",
    "latent_dim = 32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Preprocessing\n",
      "Found 14630 files belonging to 3 classes.\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset using TensorFlow functions\n",
    "print(\"Train Preprocessing\")\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_PATH + '/train',\n",
    "    image_size=(width, width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'\n",
    ")\n",
    "print(type(train_dataset))\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation preprocessing\n",
      "Found 1500 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"validation preprocessing\")\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_PATH + '/val',\n",
    "    image_size=(width, width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = models.Sequential([\n",
    "            layers.InputLayer(input_shape=input_dim),\n",
    "            layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim + latent_dim),  # Two times latent_dim for mean and log-variance\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = models.Sequential([\n",
    "            layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            layers.Dense(16 * 16 * 128, activation=\"relu\"),\n",
    "            layers.Reshape((16, 16, 128)),\n",
    "            layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\", activation=\"sigmoid\"),\n",
    "        ])\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        eps = tf.random.normal(shape=tf.shape(std))\n",
    "        return mu + eps * std\n",
    "\n",
    "    def call(self, x):\n",
    "        enc_output = self.encoder(x)\n",
    "        mu, logvar = enc_output[:, :latent_dim], enc_output[:, latent_dim:]\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        dec_output = self.decoder(z)\n",
    "        return dec_output, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "# # Instantiate the VAE model\n",
    "# print(\"VAE Class Object\")\n",
    "# vae = VAE(latent_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(294, 128, 128, 3)\n",
      "Total NaN values in images: 0\n",
      "Total NaN values in labels: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming train_dataset is already defined\n",
    "nan_count_images = 0\n",
    "nan_count_labels = 0\n",
    "\n",
    "for images, labels in train_dataset:\n",
    "    print(images.shape)\n",
    "    # Check for NaN values in images\n",
    "    nan_mask_images = tf.math.is_nan(images)\n",
    "    nan_count_images += tf.reduce_sum(tf.cast(nan_mask_images, tf.int32)).numpy()\n",
    "\n",
    "    # Cast labels to float32 before checking for NaN values\n",
    "    labels_float32 = tf.cast(labels, tf.float32)\n",
    "    nan_mask_labels = tf.math.is_nan(labels_float32)\n",
    "    nan_count_labels += tf.reduce_sum(tf.cast(nan_mask_labels, tf.int32)).numpy()\n",
    "\n",
    "# Print the total count of NaN values in images and labels\n",
    "print(\"Total NaN values in images:\", nan_count_images)\n",
    "print(\"Total NaN values in labels:\", nan_count_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count in recon_data: 0\n",
      "NaN count in mu: 0\n",
      "NaN count in logvar: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Assume your VAE model is already defined and compiled\n",
    "vae = VAE(input_dim=(width, width, 3), latent_dim=32)\n",
    "\n",
    "\n",
    "for images, labels in train_dataset:\n",
    "    # Forward pass through the VAE model\n",
    "    recon_data, mu, logvar = vae(images)\n",
    "    #print(recon_data, mu, logvar)\n",
    "    # Check for NaN values in recon_data, mu, and logvar\n",
    "    nan_mask_recon = tf.math.is_nan(recon_data)\n",
    "    nan_mask_mu = tf.math.is_nan(mu)\n",
    "    nan_mask_logvar = tf.math.is_nan(logvar)\n",
    "\n",
    "    # Count NaN values in each batch\n",
    "    count_nan_recon = tf.reduce_sum(tf.cast(nan_mask_recon, tf.int32))\n",
    "    count_nan_mu = tf.reduce_sum(tf.cast(nan_mask_mu, tf.int32))\n",
    "    count_nan_logvar = tf.reduce_sum(tf.cast(nan_mask_logvar, tf.int32))\n",
    "\n",
    "    print(f\"NaN count in recon_data: {count_nan_recon.numpy()}\")\n",
    "    print(f\"NaN count in mu: {count_nan_mu.numpy()}\")\n",
    "    print(f\"NaN count in logvar: {count_nan_logvar.numpy()}\")\n",
    "    \n",
    "    # Optionally, break the loop after checking the first batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reshape x to match recon_x's shape\n",
    "    x_reshaped = tf.reshape(x, shape=(-1,128, 128, 3))\n",
    "\n",
    "    # Calculate binary cross-entropy loss along spatial dimensions\n",
    "    BCE = tf.reduce_sum(tf.keras.losses.binary_crossentropy(recon_x, x_reshaped), axis=(1, 2))\n",
    "\n",
    "    # Calculate Kullback-Leibler Divergence\n",
    "    KLD = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar), axis=1)\n",
    "\n",
    "    return tf.reduce_mean(BCE + KLD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Epoch [1/1], Train Loss: nan\n",
      "Epoch [1/1], Validation Loss: nan\n",
      "Saving Model\n",
      "INFO:tensorflow:Assets written to: /data/home/vvaibhav/AI/VAE/vae_model/assets\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/data/home/vvaibhav/AI/VAE/VAseAFHQTensorflow.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.192.12.61/data/home/vvaibhav/AI/VAE/VAseAFHQTensorflow.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m vae\u001b[39m.\u001b[39msave(save_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.192.12.61/data/home/vvaibhav/AI/VAE/VAseAFHQTensorflow.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Generate new images\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.192.12.61/data/home/vvaibhav/AI/VAE/VAseAFHQTensorflow.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m vae\u001b[39m.\u001b[39;49meval()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.192.12.61/data/home/vvaibhav/AI/VAE/VAseAFHQTensorflow.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m random_latent \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(shape\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, latent_dim))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.192.12.61/data/home/vvaibhav/AI/VAE/VAseAFHQTensorflow.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m generated_images \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mdecoder(random_latent)\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Started\")\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataset:\n",
    "        data, _ = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            recon_data, mu, logvar = vae(data)\n",
    "            loss = loss_function(recon_data, data, mu, logvar)\n",
    "            print(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, vae.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
    "\n",
    "        total_train_loss += loss.numpy()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation loop\n",
    "    total_val_loss = 0\n",
    "    for val_batch in val_dataset:\n",
    "        val_data, _ = val_batch\n",
    "        recon_val_data, mu_val, logvar_val = vae(val_data)\n",
    "        val_loss = loss_function(recon_val_data, val_data, mu_val, logvar_val)\n",
    "        total_val_loss += val_loss.numpy()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "# Save the model after training\n",
    "print(\"Saving Model\")\n",
    "save_path = \"/data/home/vvaibhav/AI/VAE/vae_model\"\n",
    "vae.save(save_path)\n",
    "\n",
    "# Generate new images\n",
    "vae.eval()\n",
    "random_latent = tf.random.normal(shape=(16, latent_dim))\n",
    "generated_images = vae.decoder(random_latent).numpy()\n",
    "\n",
    "# Display or save the generated images as needed\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    image = generated_images[i]\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
